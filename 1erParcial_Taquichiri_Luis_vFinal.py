# -*- coding: utf-8 -*-
"""1erParcial_Taquichiri_Luis_v.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bl0pduEe1oyNydqnIwkIChkzq_92LWnx

#1er Parcial COM300 IA
Taquichiri Huarita Luis Alexander

####Importación de bibliotecas
"""

# Commented out IPython magic to ensure Python compatibility.
#Para interactuar con el sistema operativo.
import os
#Para la manipulación y análisis de datos tabulares (DataFrames).
import pandas as pd
#Para realizar operaciones matemáticas y manipulación de arreglos/matrices.
import numpy as np
#Para la creación de gráficos y visualización de datos.
from matplotlib import pyplot
import matplotlib.pyplot as plt
# %matplotlib inline

"""Cargar y revisar el data set"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/machine learning/datasets/Player_Attributes.csv', delimiter=',', decimal='.')
data = data.dropna()
data.info()

"""####Arreglo del Data set

Cargar datos
"""

#Cargar los datos en un data set limpio y más fácil de manejar
nuevo_data = data.iloc[:20000, [4,5,9,10,11,12,13,14,15,16,17]]
#convertir los datos para tener clases dividiendo los jugadores en: muy malos, malos, neutro, buenos y muy buenos (0,1,2,3,4)
first = np.zeros(nuevo_data.shape[0])
for i in range(nuevo_data.shape[0]):
  if nuevo_data.iloc[i,0] <= 20:
    first[i] = 0
  elif nuevo_data.iloc[i,0] <= 40:
    first[i] = 1
  elif nuevo_data.iloc[i,0] <= 60:
    first[i] = 2
  elif nuevo_data.iloc[i,0] <= 80:
    first[i] = 3
  else:
    first[i] = 4
#print(nuevo_data.iloc[100:200,0])
nuevo_data.iloc[:,0] = first
#print(nuevo_data.iloc[100:200,0])

print(nuevo_data)

"""Insertar atributos de: carácter, texto y valores booleanos"""

def agregarCTB(data):
  # Agregar atributos de texto (posición del jugador)
  positions = ['Defensor', 'MedioCampista', 'Delantero', 'Portería']
  data['Posición'] = np.random.choice(positions, data.shape[0])
  # Agregar atributos de caracteres (Grupo: A, B, C, D)
  Groups = ['A', 'B', 'C', 'D']
  data['Grupo'] = np.random.choice(Groups, data.shape[0])
  # Agregar atributos booleanos (¿Es titular?)
  data['Es titular?'] = np.random.choice([True, False], data.shape[0])

agregarCTB(nuevo_data)
print(nuevo_data)

"""Duplicado de los datos del data set"""

#como los datos son de jugadores de futbol y no hay forma de hacer una corelación exacta usaré
#un numero random para aumentar o restar del valor original haciendo una variación que no afecte mucho
#manteniendo las clases y ramdomificando los datos de otro tipo con la función
data_duplicado = pd.DataFrame()
data_duplicado['overall_rating'] = nuevo_data.iloc[:,0]
for j in range(1,11):
  temp = []
  for i in range(nuevo_data.shape[0]):
    temp.append(nuevo_data.iloc[i, j] + np.random.uniform(-3, 3))
  data_duplicado[nuevo_data.columns[j]] = temp

agregarCTB(data_duplicado)
print(data_duplicado)

"""Función para visualizar datos en un eje"""

def plotData(x, y):
    #Grafica los puntos x e y en una figura nueva.

    fig = pyplot.figure()  # abre una nueva figura

    pyplot.plot(x, y, 'ro', ms=10, mec='k')
    pyplot.ylabel('Centros')
    pyplot.xlabel('Potencial')

"""Funcion para graficar distribuciones y la disperción"""

def comparar_distribuciones(original_data, duplicado_data, columnas, num_bins=50):
    for columna in columnas:
        plt.figure(figsize=(10, 6))
        # Histograma para el dataset original
        plt.hist(original_data[columna], bins=num_bins, alpha=0.5, label='Original', color='blue')
        # Histograma para el dataset duplicado
        plt.hist(duplicado_data[columna], bins=num_bins, alpha=0.5, label='Duplicado', color='green')

        plt.title(f'Comparación de distribuciones para {columna}')
        plt.xlabel(columna)
        plt.ylabel('Frecuencia')
        plt.legend()
        plt.show()
def comparar_dispersión(original_data, duplicado_data, columna):
    plt.figure(figsize=(10, 6))
    plt.scatter(original_data[columna], duplicado_data[columna], alpha=0.5)
    plt.title(f'Comparación de dispersión para {columna}')
    plt.xlabel('Original')
    plt.ylabel('Duplicado')
    plt.grid(True)
    plt.show()

plotData(nuevo_data.iloc[:,0], nuevo_data.iloc[:,1])
plotData(data_duplicado.iloc[:,0], data_duplicado.iloc[:,1])
plotData(nuevo_data.iloc[:1000,1], nuevo_data.iloc[:1000,2])
plotData(data_duplicado.iloc[:1000,1], data_duplicado.iloc[:1000,2])

# Seleccionar algunas columnas para comparar
columnas_a_comparar = ['overall_rating', 'potential', 'crossing']

# Comparar distribuciones para las columnas seleccionadas
comparar_distribuciones(nuevo_data, data_duplicado, columnas_a_comparar)

# Comparar la dispersión para una columna específica
comparar_dispersión(nuevo_data, data_duplicado, 'crossing')

"""Juntar los dataset inicial y sintetico en un solo data set"""

def intercalar_filas(df1, df2):
    # Concatenar los DataFrames fila por fila alternadamente
    df_combined = pd.DataFrame()
    for i in range(df1.shape[0]):
        df_combined = pd.concat([df_combined, df1.iloc[[i]], df2.iloc[[i]]], ignore_index=True)
    return df_combined

data_final = intercalar_filas(nuevo_data, data_duplicado)
print(data_final)

"""####Cargar datos en variables para usar

Voy a ignorar los datos de caracteres, texto y boleanos ya que fueron agregados de forma randomica y pueden afectar de manera negativa a las prediciciónes además que son datos que no se pueden procesar en una regresión se tendrían que cambiar a datos numericos, como perjudican más de lo que ayudan en la precisión de entrenamiento los dejaré fuera
"""

X = data_final.iloc[:32000, 1:11]
y = data_final.iloc[:32000, 0]
m = y.size
X_prueb = data_final.iloc[32000:40000, 1:11]
y_prueb = data_final.iloc[32000:40000, 0]
m_prueb = y_prueb.size
num_labels = 5
print (X)
print (y)
print (X_prueb)
print (y_prueb)

"""Función de normalización"""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])
    #Calcula la media de cada columna
    mu = np.mean(X, axis = 0)
    # Calcula la desviación estándar de cada columna. La desviación estándar mide qué tan dispersos están los datos de la media.
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

"""Normalización de datos de entrenamiento y prueba"""

X_norm, mu, sigma = featureNormalize(X)
X_prueb_norm, mu_prueb, sigma_prueb = featureNormalize(X_prueb)

"""Concatenamos x0 para los datos de entrenamiento y los datos de prueba"""

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
X_prueb = np.concatenate([np.ones((m_prueb, 1)), X_prueb], axis=1)

"""##Modelo de regresión logistica

Función de activación Sigmoide
"""

def sigmoid(z):
    z = np.array(z)
    g = np.zeros(z.shape)
    g = 1 / (1 + np.exp(-z))
    return g

"""Función de costo de la regresión lineal logistica

$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left[ -y^{(i)} \log\left(h_\theta\left( x^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - h_\theta\left( x^{(i)} \right) \right) \right]$$

es la función binary cross entropy
"""

def calcularCosto(theta, X, y):
    m = y.size
    J = 0
    h = sigmoid(X.dot(theta.T))
    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))

    return J

"""Función que por medio de repeticiones logra sacar la mejor theta posible mediante el descenso por la gradiente"""

def descensoGradiente(theta, X, y, alpha, num_iters):
    m = y.shape[0]
    theta = theta.copy()
    J_history = []

    for i in range(num_iters):
        h = sigmoid(X.dot(theta.T))
        #Actualiza los parametros de theta con la formula del descenso por la gradiente
        theta = theta - (alpha / m) * (h - y).dot(X)
        J_history.append(calcularCosto(theta, X, y))

    return theta, J_history

"""Función que saca las precciones con sigmoide de los datos que le pasemos"""

def prediccion(theta, X):
    m = X.shape[0]
    p = np.zeros(m)
    p = np.round(sigmoid(X.dot(theta.T)))
    return p

"""Función para comparar prediciones ejemplo"""

def prediccionEj(y, yp, indices):
    m = indices.shape[0]
    for i in range(m):
      print('La predicción es: ',yp[indices[i]],'El valor original es de: ', y[indices[i]])

"""###Ejecución de las funciones en cada caso y su prueba de eficacia

funcion que devuelve un array de datos convertidoa a 0 o 1 según la clase
"""

def covertirbin(clase,x):
  m = x.shape[0]
  yb = np.zeros(m)
  for i in range(m):
    if x.iloc[i] == clase:
      yb[i] = 1
    else:
      yb[i] = 0
  return yb

"""####Primera clase (0)"""

y_0 = covertirbin(0,y)
y_prueb_0 = covertirbin(0,y_prueb)

alpha = 0.02
num_iters = 6000
theta_0 = np.zeros(X.shape[1])
theta_0, J_history = descensoGradiente(theta_0, X, y_0, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('ultimo J: ',J_history[-1])
print('--------------------------------------------------------------------------')
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_0)))
print('--------------------------------------------------------------------------')

p = prediccion(theta_0, X)
print('Precisión de entrenamiento: {:.2f} %'.format(np.mean(p == y_0) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj( y_0, p, indices)

p = prediccion(theta_0, X_prueb)
print('Precisión de los datos de prueba: {:.2f} %'.format(np.mean(p == y_prueb_0) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj( y_prueb_0, p, indices)

"""####Segunda clase (1)"""

y_1 = covertirbin(1,y)
y_prueb_1 = covertirbin(1,y_prueb)

alpha = 0.03
num_iters = 5000
theta_1 = np.zeros(X.shape[1])
theta_1, J_history = descensoGradiente(theta_1, X, y_1, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('ultimo J: ',J_history[-1])
print('--------------------------------------------------------------------------')
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_1)))
print('--------------------------------------------------------------------------')

p = prediccion(theta_1, X)
print('Precisión de entrenamiento: {:.2f} %'.format(np.mean(p == y_1) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_1, p, indices)

p = prediccion(theta_1, X_prueb)
print('Precisión de los datos de prueba: {:.2f} %'.format(np.mean(p == y_prueb_1) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_prueb_1, p, indices)

"""####Tercera clase (2)"""

y_2 = covertirbin(2,y)
y_prueb_2 = covertirbin(2,y_prueb)

alpha = 0.02
num_iters = 6000
theta_2 = np.zeros(X.shape[1])
theta_2, J_history = descensoGradiente(theta_2, X, y_2, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('ultimo J: ',J_history[-1])
print('--------------------------------------------------------------------------')
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_2)))
print('--------------------------------------------------------------------------')

p = prediccion(theta_2, X)
print('Precisión de entrenamiento: {:.2f} %'.format(np.mean(p == y_2) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj( y_2, p, indices)

p = prediccion(theta_2, X_prueb)
print('Precisión de los datos de prueba: {:.2f} %'.format(np.mean(p == y_prueb_2) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_prueb_2, p, indices)

"""####Cuarta clase (3)"""

y_3 = covertirbin(3,y)
y_prueb_3 = covertirbin(3,y_prueb)

alpha = 0.01
num_iters = 6000
theta_3 = np.zeros(X.shape[1])
theta_3, J_history = descensoGradiente(theta_3, X, y_3, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('ultimo J: ',J_history[-1])
print('--------------------------------------------------------------------------')
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_3)))
print('--------------------------------------------------------------------------')

p = prediccion(theta_3, X)
print('Precisión de entrenamiento: {:.2f} %'.format(np.mean(p == y_3) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_3, p, indices)

p = prediccion(theta_3, X_prueb)
print('Precisión de los datos de prueba: {:.2f} %'.format(np.mean(p == y_prueb_3) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_prueb_3, p, indices)

"""####Quinta clase (4)"""

y_4 = covertirbin(4,y)
y_prueb_4 = covertirbin(4,y_prueb)

alpha = 0.02
num_iters = 10000
theta_4 = np.zeros(X.shape[1])
theta_4, J_history = descensoGradiente(theta_4, X, y_4, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('ultimo J: ',J_history[-1])
print('--------------------------------------------------------------------------')
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_4)))
print('--------------------------------------------------------------------------')

p = prediccion(theta_4, X)
print('Precisión de entrenamiento: {:.2f} %'.format(np.mean(p == y_4) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_4, p, indices)

p = prediccion(theta_4, X_prueb)
print('Precisión de los datos de prueba: {:.2f} %'.format(np.mean(p == y_prueb_4) * 100))
print('--------------------------------------------------------------------------')
print('50 predicciones aleatorias: ')
indices = np.random.randint(0, 4000, 50)
prediccionEj(y_prueb_4, p, indices)

"""##Modelo One vs All

Funcion que calcula el costo binary cross entropy más término de regularización

$$ J(\theta) = \frac{1}{m} \sum_{i=1}^m \left[ -y^{(i)} \log \left(h_\theta\left(x^{(i)} \right)\right) - \left( 1 - y^{(i)} \right) \log\left(1 - h_\theta \left(x^{(i)} \right) \right) \right] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2 $$


y entrena los thetas
"""

def lrCostFunction(theta, X, y, lambda_):
    m = y.size
    h = sigmoid(X.dot(theta))

    # Función de costo binary cross entropy con regularización
    cost = (1 / m) * np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h)) + (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))

    # Gradiente con regularización
    grad = (1 / m) * X.T.dot(h - y)
    grad[1:] += (lambda_ / m) * theta[1:]

    return cost, grad

"""Descenso por la gradiente para calcular por numpy"""

def gradientDescent(X, y, initial_theta, alpha, num_iters, lambda_):

    theta = initial_theta.copy()
    m = y.size
    J_history = []

#Hace iteraciones para encontrar los theta optimos con los parametros que le pasemos
    for _ in range(num_iters):
        #aplicación de la funcion de costo
        cost, grad = lrCostFunction(theta, X, y, lambda_)
        #Minimización de la función de costo: El gradiente grad apunta en la dirección de mayor aumento de la función de costo.
        #Al restar este gradiente (theta -= ...), estamos actualizando los parámetros en la dirección opuesta,
        #lo que ayuda a minimizar la función de costo.
        theta -= alpha * grad
        #Guarda el costo del calculo de theta para x clase
        J_history.append(cost)

    return theta, J_history

"""Funcion para calcular todas las thetas es decir las diferentes thetas para cada clase

Entrena un clasificador de regresión logística para un problema de clasificación multiclase usando el enfoque One-vs-All
"""

def oneVsAll(X, y, num_labels, alpha, num_iters, lambda_):

    m, n = X.shape
    all_theta = np.zeros((num_labels, n))
    J_history_all = []

    for c in range(num_labels):
      #muesta mensaje indicando el numero de clase
      print(f"Entrenando clase {c+1} de {num_labels}...")
      #convierte los datos en boleanos y luego en enteros de nuevo dependiendo de la clase que estemos evaluando
      y_c = (y == c).astype(int)
      initial_theta = np.zeros(n)

      theta, J_history = gradientDescent(X, y_c, initial_theta, alpha, num_iters, lambda_)
      #agrega los thetas encontrados al total de thetas
      all_theta[c] = theta
      #total de costos
      J_history_all.append(J_history)

      # Graficar J_history para la clase actual
      plt.plot(J_history, label=f'Clase {c+1}')

    plt.xlabel('Número de Iteraciones')
    plt.ylabel('Costo')
    plt.title('Historial del Costo para cada Clase')
    plt.legend()
    plt.show()

    return all_theta

"""Aplicación del las funciones"""

alpha = 0.01
num_iters = 10000
lambda_ = 0.01
all_theta = oneVsAll(X, y, num_labels, alpha, num_iters, lambda_)

print('thetas one vs all: ',all_theta)
print('---------------------------------------------')
print('thetas regreción logistica: ')
print(theta_0)
print(theta_1)
print(theta_2)
print(theta_3)
print(theta_4)

"""Función para predecir one vs All

"""

def predictOneVsAll(all_theta, X):
    m = X.shape[0];
    num_labels = all_theta.shape[0]

    p = np.zeros(m)
    # Calcula las probabilidades de cada clase usando la función sigmoide y el producto de matrices
    # entre X y los parámetros transpuestos (all_theta.T)
    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)
    #La función np.argmax selecciona la clase con la probabilidad más alta para cada ejemplo.
    #axis=1 indica que la búsqueda se realiza a lo largo del eje de las clases.
    return p

"""Calculo de efectividad con los datos de entrenamiento"""

pred = predictOneVsAll(all_theta, X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))
print("------------------------------------------------------------------------")
print("Datos comparados:")
indices = np.random.randint(0, 4000, 50)
prediccionEj(y, pred, indices)

"""Calculo de efectividad con los datos de prueba"""

pred_prueba = predictOneVsAll(all_theta, X_prueb)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred_prueba == y_prueb) * 100))
print("------------------------------------------------------------------------")
print("Datos comparados:")
prediccionEj(y_prueb, pred_prueba, indices)